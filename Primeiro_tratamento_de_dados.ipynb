{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Djulliecbr/ModelosKNN-SVM/blob/main/Primeiro_tratamento_de_dados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY7aNuiSBJbr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Carregar os arquivos CSV de treino e teste\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Visualizar as primeiras linhas dos conjuntos de treino e teste para entender os dados\n",
        "train_head = train_df.head()\n",
        "test_head = test_df.head()\n",
        "\n",
        "# Informações gerais sobre os datasets\n",
        "train_info = train_df.info()\n",
        "test_info = test_df.info()\n",
        "\n",
        "(train_head, test_head, train_info, test_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remover todas as linhas que possuem valores ausentes\n",
        "dataset_train_cleaned = train_df.dropna()\n",
        "dataset_test_cleaned = test_df.dropna()\n",
        "\n",
        "# Exibir o número de linhas antes e depois da remoção\n",
        "print(\"Número de linhas do conjunto de treino antes de remover valores ausentes:\", train_df.shape[0])\n",
        "print(\"Número de linhas após remover valores ausentes do dataset de treino:\", dataset_train_cleaned.shape[0])\n",
        "print('------------------------------------------------------------------------------------------------------')\n",
        "print(\"Número de linhas do conjunto de teste antes de remover valores ausentes:\", test_df.shape[0])\n",
        "print(\"Número de linhas após remover valores ausentes do dataset de testes:\", dataset_test_cleaned.shape[0])\n"
      ],
      "metadata": {
        "id": "Dkb8gMmlgPMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analisando a porcentagem de valores ausentes em cada coluna do conjunto de treino\n",
        "missing_values_percentage = train_df.isnull().mean() * 100\n",
        "\n",
        "# Removendo colunas com mais de 50% de valores ausentes\n",
        "columns_to_drop = missing_values_percentage[missing_values_percentage > 50].index\n",
        "train_df_cleaned = train_df.drop(columns=columns_to_drop)\n",
        "test_df_cleaned = test_df.drop(columns=columns_to_drop)\n",
        "\n",
        "# Preenchendo os valores ausentes nas colunas restantes com a mediana de cada coluna\n",
        "train_df_cleaned = train_df_cleaned.fillna(train_df_cleaned.median())\n",
        "test_df_cleaned = test_df_cleaned.fillna(test_df_cleaned.median())"
      ],
      "metadata": {
        "id": "CPasggN6EEy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separando as features (X) e o rótulo (y) nos conjuntos de treino e teste\n",
        "X_train = train_df_cleaned.drop(columns=['y'])\n",
        "y_train = train_df_cleaned['y']\n",
        "X_test = test_df_cleaned.drop(columns=['y'])\n",
        "y_test = test_df_cleaned['y']\n",
        "\n",
        "# Normalizando os dados (StandardScaler para padronização)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "d_LJLbrnEKbx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}